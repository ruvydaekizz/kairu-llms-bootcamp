# ğŸ“ Hafta 3 Ã–dev - Model Performans Analizi

**Teslim Tarihi**: 1 Hafta  
**Toplam Puan**: 100 puan  
**Format**: Python kodu + KÄ±sa rapor

## ğŸ¯ Ã–dev AmacÄ±

Bu Ã¶devde, 3 farklÄ± model tÃ¼rÃ¼nÃ¼ (GPT, BERT, T5) aynÄ± metin Ã¼zerinde test ederek performanslarÄ±nÄ± karÅŸÄ±laÅŸtÄ±racak bir analiz programÄ± geliÅŸtireceksiniz.

## ğŸ“‹ GÃ¶rev: "LLM Model Performance Analyzer"

### YapmanÄ±z Gereken:

3 farklÄ± model ile aynÄ± metinleri test edip performanslarÄ±nÄ± karÅŸÄ±laÅŸtÄ±ran **tek bir program** yazÄ±n:

- **GPT-2**: Text generation  
- **BERT**: Sentiment analysis
- **T5**: Text summarization

Her model iÃ§in Ã¶lÃ§meniz gerekenler:
- â±ï¸ Ã‡alÄ±ÅŸma sÃ¼resi
- ğŸ’¾ Memory kullanÄ±mÄ±  
- ğŸ“Š Ã‡Ä±ktÄ± kalitesi

## ğŸ”§ Beklenen Ã‡Ä±ktÄ± FormatÄ±:

```
ğŸ¤– Model Performance Analyzer
========================================
Test Text: "I love programming with Python. It's amazing for AI!"

ğŸ“Š GPT-2 Results:
- Task: Text Generation
- Time: 0.45 seconds
- Memory: 2.1 GB
- Output: "I love programming with Python. It's amazing for AI! The language..."

ğŸ“Š BERT Results:  
- Task: Sentiment Analysis
- Time: 0.23 seconds
- Memory: 1.8 GB
- Output: POSITIVE (confidence: 0.94)

ğŸ“Š T5 Results:
- Task: Summarization  
- Time: 0.67 seconds
- Memory: 2.3 GB
- Output: "Python programming is great for AI development."

ğŸ† Performance Summary:
- Fastest Model: BERT (0.23s)
- Most Memory Efficient: BERT (1.8GB)
- Best for Generation: GPT-2
- Best for Analysis: BERT
- Best for Summarization: T5

ğŸ’¡ Recommendation:
For quick sentiment analysis, use BERT.
For creative text generation, use GPT-2.
For summarizing long texts, use T5.
```

## ğŸš€ BaÅŸlangÄ±Ã§ Kodu

```python
import torch
import time
import psutil
from transformers import pipeline

def measure_performance(model_pipeline, text, task_name):
    """Model performansÄ±nÄ± Ã¶lÃ§er"""
    start_time = time.time()
    start_memory = psutil.virtual_memory().used / 1e9
    
    # Model Ã§alÄ±ÅŸtÄ±r
    if task_name == "text_generation":
        result = model_pipeline(text, max_length=50, num_return_sequences=1, do_sample=False)
    elif task_name == "sentiment_analysis":
        result = model_pipeline(text)
    elif task_name == "summarization":
        result = model_pipeline(text, max_length=30, min_length=10, do_sample=False)
    else:
        result = model_pipeline(text)
    
    end_time = time.time()
    end_memory = psutil.virtual_memory().used / 1e9
    
    return {
        'task': task_name,
        'time': round(end_time - start_time, 3),
        'memory': round(end_memory, 2), 
        'result': result
    }

def main():
    # Test metni
    test_text = "I love programming with Python. It's amazing for AI development and machine learning projects!"
    
    print("ğŸ¤– Model Performance Analyzer")
    print("=" * 40)
    print(f"Test Text: {test_text}\n")
    
    # TODO: Buradan itibaren siz tamamlayacaksÄ±nÄ±z!
    
    # 1. GPT-2 Pipeline oluÅŸturun
    print("ğŸ“¦ Loading GPT-2...")
    # gpt2_pipeline = pipeline("text-generation", model="gpt2")
    
    # 2. BERT Pipeline oluÅŸturun  
    print("ğŸ“¦ Loading BERT...")
    # bert_pipeline = pipeline("sentiment-analysis", model="distilbert-base-uncased-finetuned-sst-2-english")
    
    # 3. T5 Pipeline oluÅŸturun
    print("ğŸ“¦ Loading T5...")
    # t5_pipeline = pipeline("summarization", model="t5-small")
    
    print("âœ… All models loaded!\n")
    
    # 4. Her modeli test edin
    print("ğŸ§ª Testing Models...")
    
    # GPT-2 Test
    # gpt2_results = measure_performance(gpt2_pipeline, test_text, "text_generation")
    
    # BERT Test  
    # bert_results = measure_performance(bert_pipeline, test_text, "sentiment_analysis")
    
    # T5 Test
    # t5_results = measure_performance(t5_pipeline, test_text, "summarization")
    
    # 5. SonuÃ§larÄ± yazdÄ±rÄ±n
    print("\nğŸ“Š Results:")
    print("-" * 40)
    
    # GPT-2 Results
    # print(f"ğŸ“Š GPT-2 Results:")
    # print(f"- Task: Text Generation")
    # print(f"- Time: {gpt2_results['time']} seconds")
    # print(f"- Memory: {gpt2_results['memory']} GB")
    # print(f"- Output: {gpt2_results['result'][0]['generated_text'][:100]}...")
    # print()
    
    # BERT Results
    # print(f"ğŸ“Š BERT Results:")
    # print(f"- Task: Sentiment Analysis")
    # print(f"- Time: {bert_results['time']} seconds")
    # print(f"- Memory: {bert_results['memory']} GB")
    # print(f"- Output: {bert_results['result'][0]['label']} (confidence: {bert_results['result'][0]['score']:.2f})")
    # print()
    
    # T5 Results
    # print(f"ğŸ“Š T5 Results:")
    # print(f"- Task: Summarization")
    # print(f"- Time: {t5_results['time']} seconds")
    # print(f"- Memory: {t5_results['memory']} GB")
    # print(f"- Output: {t5_results['result'][0]['summary_text']}")
    # print()
    
    # 6. Performance Summary
    # print("ğŸ† Performance Summary:")
    # print("-" * 40)
    # Hangi model en hÄ±zlÄ±, hangi model en az memory kullanÄ±yor vs.
    
    # 7. Recommendation
    # print("ğŸ’¡ Recommendation:")
    # print("-" * 40)
    # Hangi model ne iÃ§in uygun, Ã¶nerileriniz
    
    print("âœ… Analiz tamamlandÄ±!")

if __name__ == "__main__":
    main()
```

## ğŸ“‹ YapmanÄ±z Gerekenler:

### 1. Kodu TamamlayÄ±n (60 puan)
- [ ] GPT-2 pipeline oluÅŸturun
- [ ] BERT pipeline oluÅŸturun
- [ ] T5 pipeline oluÅŸturun
- [ ] Her modeli test edin
- [ ] SonuÃ§larÄ± formatlÄ± yazdÄ±rÄ±n
- [ ] Performance karÅŸÄ±laÅŸtÄ±rmasÄ± yapÄ±n

### 2. Analiz Ekleyin (25 puan)
- [ ] Hangi model en hÄ±zlÄ±?
- [ ] Hangi model en az memory kullanÄ±yor?
- [ ] Ã‡Ä±ktÄ± kalitesi nasÄ±l?
- [ ] Her model ne iÃ§in uygun?

### 3. Kod Kalitesi (15 puan)
- [ ] Kod Ã§alÄ±ÅŸÄ±yor ve hata vermiyor
- [ ] Yorumlar ve aÃ§Ä±klamalar var
- [ ] Temiz ve okunabilir kod

## ğŸ’¡ Ä°puÃ§larÄ±

### Teknik Ä°puÃ§larÄ±:
```python
# Pipeline oluÅŸturma
pipeline("text-generation", model="gpt2")
pipeline("sentiment-analysis", model="distilbert-base-uncased-finetuned-sst-2-english")  
pipeline("summarization", model="t5-small")

# Memory Ã¶lÃ§Ã¼mÃ¼
psutil.virtual_memory().used / 1e9  # GB cinsinden

# SÃ¼re Ã¶lÃ§Ã¼mÃ¼
start_time = time.time()
# ... iÅŸlem ...
duration = time.time() - start_time
```

### Analiz Ä°puÃ§larÄ±:
- Ä°lk Ã§alÄ±ÅŸtÄ±rma model yÃ¼kleme iÃ§erir, onu hariÃ§ tutun
- AynÄ± metni birkaÃ§ kez test edin, tutarlÄ± mÄ±?
- Hangi model hangi gÃ¶rev iÃ§in optimize edilmiÅŸ?

### YaygÄ±n Hatalar:
- Model isimlerini yanlÄ±ÅŸ yazmak
- Memory Ã¶lÃ§Ã¼mÃ¼nÃ¼ yanlÄ±ÅŸ yapmak  
- SonuÃ§larÄ± yanlÄ±ÅŸ parse etmek

## ğŸ“Š DeÄŸerlendirme Kriterleri

| Kriter | Puan | AÃ§Ä±klama |
|--------|------|----------|
| **Kod Ã‡alÄ±ÅŸÄ±yor** | 40 puan | TÃ¼m modeller yÃ¼kleniyor ve Ã§alÄ±ÅŸÄ±yor |
| **Performance Ã–lÃ§Ã¼mÃ¼** | 20 puan | SÃ¼re ve memory doÄŸru Ã¶lÃ§Ã¼lÃ¼yor |
| **Analiz ve KarÅŸÄ±laÅŸtÄ±rma** | 25 puan | Modeller objektif karÅŸÄ±laÅŸtÄ±rÄ±lmÄ±ÅŸ |
| **Kod Kalitesi** | 15 puan | Temiz, yorumlu, okunabilir kod |

## ğŸ¯ Bonus Puanlar (+10)

- [ ] **GÃ¶rselleÅŸtirme**: Matplotlib ile performans grafiÄŸi (+5 puan)
- [ ] **Extra Test**: FarklÄ± metinlerle test (+3 puan)  
- [ ] **Error Handling**: Try-catch ile hata yÃ¶netimi (+2 puan)

## ğŸ“š FaydalÄ± Kaynaklar

- [Hugging Face Pipeline Documentation](https://huggingface.co/docs/transformers/pipeline_tutorial)
- [psutil Documentation](https://psutil.readthedocs.io/)
- Hafta 3 ders materyalleri

## â“ SÄ±k Sorulan Sorular

**S: Model indirme Ã§ok yavaÅŸ?**  
C: Ä°lk seferde modeller indirilir, normal. KÃ¼Ã§Ã¼k modelleri kullanÄ±n.

**S: Memory hatasÄ± alÄ±yorum?**  
C: Daha kÃ¼Ã§Ã¼k modeller deneyin: `distilgpt2`, `distilbert-base-uncased`

**S: SonuÃ§lar her seferde farklÄ± Ã§Ä±kÄ±yor?**  
C: `do_sample=False` parametresini kullanÄ±n, daha tutarlÄ± olur.

**S: Hangi modelleri kullanmalÄ±yÄ±m?**  
C: 
- GPT: `gpt2` veya `distilgpt2`
- BERT: `distilbert-base-uncased-finetuned-sst-2-english`
- T5: `t5-small`

## ğŸ“ Teslim FormatÄ±

**Dosya adÄ±**: `isim_soyisim_hafta3.py`

**Email konusu**: "[Hafta 3 Ã–dev] Ä°sim Soyisim"

**Ä°Ã§erik**:
1. Python dosyasÄ± (.py)
2. KÄ±sa rapor (2-3 paragraf):
   - Hangi model ne iÃ§in uygun?
   - En Ã§ok neyi Ã¶ÄŸrendiniz?
   - GerÃ§ek dÃ¼nyada nasÄ±l kullanÄ±rsÄ±nÄ±z?

---

**Ä°yi Ã§alÄ±ÅŸmalar! ğŸš€**

*Bu Ã¶dev, gerÃ§ek dÃ¼nya LLM uygulamalarÄ±nda model seÃ§imi konusunda deneyim kazandÄ±racak.*
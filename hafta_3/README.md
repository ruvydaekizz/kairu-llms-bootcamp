# Hafta 3: AutoTokenizer, AutoModel ve Pipeline Optimizasyonu

Bu hafta, Hugging Face Transformers kÃ¼tÃ¼phanesinin temel bileÅŸenlerini derinlemesine inceleyerek model performansÄ±nÄ± optimize etme yÃ¶ntemlerini Ã¶ÄŸreniyoruz.

## ğŸ“š Ä°Ã§erik

### 1. AutoTokenizer & AutoModel YapÄ±sÄ± + Pipeline ile HÄ±zlÄ± Model Ã‡aÄŸÄ±rma
**Dosya:** `01_autotokenizer_automodel.py`

- AutoTokenizer kullanÄ±mÄ± ve tokenization iÅŸlemleri
- AutoModel ile manuel model Ã§aÄŸÄ±rma
- Pipeline ile hÄ±zlÄ± model kullanÄ±mÄ±
- Manual vs Pipeline performans karÅŸÄ±laÅŸtÄ±rmasÄ±
- Ã–zelleÅŸtirilmiÅŸ pipeline Ã¶rnekleri

**Ana Konular:**
- Tokenization (encode/decode)
- Model inference
- Pipeline kullanÄ±mÄ± (sentiment-analysis, text-generation, qa, fill-mask)
- Feature extraction
- Performans optimizasyonu

### 2. GPT, BERT ve T5 Modellerinin FarklarÄ± ve Pipeline Entegrasyonu
**Dosya:** `02_gpt_bert_t5_comparison.py`

- Model mimarilerinin karÅŸÄ±laÅŸtÄ±rmasÄ±
- Her modelin gÃ¼Ã§lÃ¼ yÃ¶nleri ve kullanÄ±m alanlarÄ±
- Pipeline ile Ã¼Ã§ modeli tek satÄ±rda test etme
- Model boyutu ve performans karÅŸÄ±laÅŸtÄ±rmasÄ±

**Model Ã–zellikleri:**

| Model | Mimari | GÃ¼Ã§lÃ¼ YÃ¶nler | KullanÄ±m AlanlarÄ± |
|-------|--------|--------------|-------------------|
| **GPT** | Decoder-only | Text generation | Creative writing, Conversational AI |
| **BERT** | Encoder-only | Bidirectional understanding | Classification, NER, QA |
| **T5** | Encoder-decoder | Text-to-text format | Translation, Summarization |

### 3. CPU/GPU Performans YÃ¶netimi ve Model Optimizasyonu
**Dosya:** `03_cpu_gpu_optimization.py`

- Cihaz tespiti ve optimal cihaz seÃ§imi
- CPU optimizasyonu (thread ayarlarÄ±)
- GPU optimizasyonu (memory management)
- Model quantization (8-bit, dynamic)
- Batch processing optimizasyonu
- Memory efficient inference

**Optimizasyon Teknikleri:**
- `torch.no_grad()` kullanÄ±mÄ±
- Memory cleanup (`torch.cuda.empty_cache()`)
- Quantization (BitsAndBytesConfig)
- Batch size optimizasyonu
- Device-specific optimizations

### 4. Pipeline ile GPU/CPU PerformansÄ±nÄ± Ã–lÃ§me ve KÄ±yaslama
**Dosya:** `04_performance_measurement.py`

- Performans Ã¶lÃ§Ã¼m araÃ§larÄ± (PerformanceMeter sÄ±nÄ±fÄ±)
- FarklÄ± task'lar iÃ§in benchmark testleri
- Batch size performans analizi
- Model karÅŸÄ±laÅŸtÄ±rma benchmark'larÄ±
- DetaylÄ± performans raporlarÄ±

**Ã–lÃ§Ã¼len Metrikler:**
- Inference sÃ¼resi
- Memory kullanÄ±mÄ± (CPU/GPU)
- Throughput (texts/second)
- Device utilization
- Model loading time

## ğŸš€ Kurulum ve Ã‡alÄ±ÅŸtÄ±rma

### ğŸ”§ Otomatik Kurulum

#### macOS / Linux
```bash
cd hafta_3
chmod +x start.sh
./start.sh
```

#### Windows
```cmd
cd hafta_3
.\start.bat
```

### ğŸ“ Manuel Kurulum

#### 1. Sanal Ortam OluÅŸtur
```bash
# macOS/Linux
python3 -m venv llm_bootcamp_env
source llm_bootcamp_env/bin/activate

# Windows
python -m venv llm_bootcamp_env
llm_bootcamp_env\Scripts\activate.bat
```

#### 2. BaÄŸÄ±mlÄ±lÄ±klarÄ± YÃ¼kle
```bash
pip install --upgrade pip
pip install -r requirements.txt
```

#### 3. ModÃ¼lleri Ã‡alÄ±ÅŸtÄ±r
```bash
# AutoTokenizer ve AutoModel Ã¶rnekleri
python 01_autotokenizer_automodel.py

# Model karÅŸÄ±laÅŸtÄ±rmasÄ±
python 02_gpt_bert_t5_comparison.py

# Performans optimizasyonu
python 03_cpu_gpu_optimization.py

# Performans Ã¶lÃ§Ã¼mÃ¼
python 04_performance_measurement.py
```

## ğŸ“‹ Gereksinimler

```bash
pip install transformers torch torchvision torchaudio
pip install psutil matplotlib numpy
pip install bitsandbytes  # Quantization iÃ§in (opsiyonel)
```

**GPU DesteÄŸi iÃ§in:**
- CUDA: `pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118`
- Apple Silicon: Otomatik olarak MPS desteÄŸi

## ğŸ¯ Ã–ÄŸrenme Hedefleri

Bu hafta sonunda ÅŸunlarÄ± Ã¶ÄŸrenmiÅŸ olacaksÄ±nÄ±z:

1. **AutoTokenizer ve AutoModel** kullanarak manual model Ã§aÄŸÄ±rma
2. **Pipeline'lar** ile hÄ±zlÄ± ve kolay model kullanÄ±mÄ±
3. **GPT, BERT, T5** model farklarÄ±nÄ± ve hangisini ne zaman kullanacaÄŸÄ±nÄ±zÄ±
4. **CPU/GPU optimizasyonu** ile performansÄ± artÄ±rma
5. **Performans Ã¶lÃ§Ã¼mÃ¼** ve benchmark yapma
6. **Model quantization** ile memory kullanÄ±mÄ±nÄ± azaltma
7. **Batch processing** ile throughput artÄ±rma

## ğŸ’¡ En Ä°yi Uygulamalar

### Performans Optimizasyonu
```python
# âœ… Ä°yi
with torch.no_grad():
    outputs = model(**inputs)

# âŒ KÃ¶tÃ¼  
outputs = model(**inputs)  # Gradient hesaplanÄ±r
```

### Device YÃ¶netimi
```python
# âœ… Ä°yi
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device)
inputs = {k: v.to(device) for k, v in inputs.items()}

# âŒ KÃ¶tÃ¼
model = model.to("cuda")  # CUDA olmayabilir
```

### Memory YÃ¶netimi
```python
# âœ… Ä°yi
del model
torch.cuda.empty_cache()
gc.collect()

# âŒ KÃ¶tÃ¼
# Memory leak'e sebep olabilir
```

## ğŸ“Š Benchmark SonuÃ§larÄ±

Tipik performans karÅŸÄ±laÅŸtÄ±rmasÄ± (Ã¶rnek sistem):

| Model | Device | Inference Time | Memory Usage |
|-------|--------|----------------|--------------|
| DistilBERT | CPU | 0.045s | 1.2 GB |
| DistilBERT | GPU | 0.012s | 2.1 GB |
| BERT-base | CPU | 0.089s | 2.1 GB |
| BERT-base | GPU | 0.021s | 3.2 GB |

## ğŸ” Sorun Giderme

### YaygÄ±n Hatalar

**1. CUDA Out of Memory**
```python
# Ã‡Ã¶zÃ¼m: Batch size'Ä± azaltÄ±n
batch_size = 8  # 32 yerine
torch.cuda.empty_cache()
```

**2. Model Loading HatasÄ±**
```python
# Ã‡Ã¶zÃ¼m: Cihaz uyumluluÄŸunu kontrol edin
device = get_optimal_device()
model = model.to(device)
```

**3. Tokenizer HatasÄ±**
```python
# Ã‡Ã¶zÃ¼m: Padding ve truncation ekleyin
inputs = tokenizer(text, return_tensors="pt", 
                   padding=True, truncation=True)
```

## ğŸ“š Ek Kaynaklar

- [Hugging Face Transformers Documentation](https://huggingface.co/docs/transformers/)
- [PyTorch Performance Tuning Guide](https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html)
- [BERT Paper](https://arxiv.org/abs/1810.04805)
- [GPT Paper](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)
- [T5 Paper](https://arxiv.org/abs/1910.10683)

## ğŸ¯ Pratik Egzersizler

1. **Kendi Modelinizi Test Edin**: FarklÄ± bir model ile performance benchmark yapÄ±n
2. **Custom Pipeline**: Kendi task'Ä±nÄ±z iÃ§in Ã¶zelleÅŸtirilmiÅŸ pipeline oluÅŸturun
3. **Optimization Challenge**: Bir modelin performansÄ±nÄ± %50 artÄ±rmaya Ã§alÄ±ÅŸÄ±n
4. **Memory Analysis**: FarklÄ± model boyutlarÄ±nÄ±n memory kullanÄ±mÄ±nÄ± analiz edin

---

**Not:** Bu modÃ¼ller eÄŸitim amaÃ§lÄ±dÄ±r. Production kullanÄ±mÄ±nda gÃ¼venlik ve error handling eklemeleri yapÄ±lmalÄ±dÄ±r.
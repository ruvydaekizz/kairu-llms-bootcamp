"""
Basit RAG (Retrieval-Augmented Generation) Demo
==============================================

Bu basitleÅŸtirilmiÅŸ Ã¶rnek RAG'in temel mantÄ±ÄŸÄ±nÄ± gÃ¶sterir:
1. ğŸ“š 3 kÄ±sa belge â†’ Chroma Vector DB'de sakla
2. ğŸ” Sorgu â†’ Vector DB'de arama
3. ğŸ“„ En yakÄ±n belgeyi bul
4. ğŸ¤– Prompt + BaÄŸlam â†’ LLM YanÄ±tÄ±

RAG SÃ¼reci Ã–zeti:
Query â†’ Vector DB Search â†’ Retrieve â†’ Augment â†’ Generate
"""

import numpy as np
from sentence_transformers import SentenceTransformer
import chromadb
import os
from dotenv import load_dotenv

# .env dosyasÄ±nÄ± yÃ¼kle
load_dotenv()

# OpenAI import
try:
    import openai
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False

print("ğŸ¤– Basit RAG Demo - 4 AdÄ±mda RAG")
print("="*40)

# ADIM 1: ğŸ“š Chroma Vector DB Setup ve Belge YÃ¼kleme
print("\nğŸ“š ADIM 1: Vector Database Setup")
print("-" * 30)

# Chroma client oluÅŸtur
client = chromadb.Client()

# Collection oluÅŸtur (eÄŸer varsa sil)
collection_name = "simple_rag_demo"
try:
    client.delete_collection(collection_name)
except:
    pass

collection = client.create_collection(
    name=collection_name,
    metadata={"hnsw:space": "cosine"}
)

# Belge koleksiyonu
documents = [
    {
        "id": "doc_1",
        "text": "Python kolay Ã¶ÄŸrenilebilen, gÃ¼Ã§lÃ¼ bir programlama dilidir. Web uygulamalarÄ±, veri analizi ve yapay zeka projelerinde kullanÄ±lÄ±r.",
        "category": "programming"
    },
    {
        "id": "doc_2", 
        "text": "JavaScript web tarayÄ±cÄ±larÄ±nda Ã§alÄ±ÅŸan dinamik bir dildir. Frontend ve backend geliÅŸtirme iÃ§in Node.js ile birlikte kullanÄ±labilir.",
        "category": "programming"
    },
    {
        "id": "doc_3",
        "text": "Machine learning algoritmalarÄ± verilerden pattern Ã¶ÄŸrenir. Supervised learning etiketli verilerle, unsupervised learning etiketsiz verilerle Ã§alÄ±ÅŸÄ±r.",
        "category": "ai"
    }
]

print(f"âœ… {len(documents)} belge hazÄ±rlandÄ±:")
for i, doc in enumerate(documents, 1):
    print(f"{i}. [{doc['category']}] {doc['text'][:50]}...")

# ADIM 2: ğŸ—„ï¸ Belgeleri Vector DB'ye YÃ¼kleme
print("\nğŸ—„ï¸ ADIM 2: Vector DB'ye YÃ¼kleme")
print("-" * 30)

# Model yÃ¼kle (Chroma otomatik embedding yapacak, ama kontrol iÃ§in)
model = SentenceTransformer('all-MiniLM-L6-v2')
print(f"âœ… Embedding model: {model.get_sentence_embedding_dimension()}D")

# Belgeleri Chroma'ya ekle
texts = [doc["text"] for doc in documents]
ids = [doc["id"] for doc in documents] 
metadatas = [{"category": doc["category"]} for doc in documents]

collection.add(
    documents=texts,
    metadatas=metadatas,
    ids=ids
)

print(f"âœ… {collection.count()} belge Vector DB'de saklandÄ±")
print(f"ğŸ“Š KullanÄ±lan similarity: cosine")

# ADIM 3: ğŸ” Vector DB'de Arama
print("\nğŸ” ADIM 3: Vector DB'de Arama")
print("-" * 30)

def search_vector_db(query, top_k=1):
    """Vector DB'de arama yap"""
    print(f"ğŸ”¤ Sorgu: '{query}'")
    
    # Chroma'da arama
    results = collection.query(
        query_texts=[query],
        n_results=top_k
    )
    
    print(f"ğŸ“Š Vector DB aramasÄ± tamamlandÄ±")
    print(f"ğŸ¯ Benzerlik skorlarÄ±: {[f'{d:.3f}' for d in results['distances'][0]]}")
    
    # En iyi sonucu al
    best_doc_id = results['ids'][0][0]
    best_score = results['distances'][0][0]
    best_document = results['documents'][0][0]
    best_metadata = results['metadatas'][0][0]
    
    print(f"ğŸ† En yakÄ±n belge: {best_doc_id} (Skor: {best_score:.3f})")
    print(f"ğŸ“‚ Kategori: {best_metadata['category']}")
    print(f"ğŸ“„ Ä°Ã§erik: {best_document[:100]}...")
    
    return {
        'id': best_doc_id,
        'text': best_document,
        'score': best_score,
        'metadata': best_metadata
    }

def answer_with_openai(prompt):
    """OpenAI ile yanÄ±t Ã¼ret"""
    if not OPENAI_AVAILABLE:
        return "âŒ OpenAI kÃ¼tÃ¼phanesi yÃ¼klÃ¼ deÄŸil"
    
    api_key = os.getenv('OPENAI_API_KEY')
    if not api_key:
        return "âŒ OPENAI_API_KEY environment variable bulunamadÄ±"
    
    try:
        client = openai.OpenAI(api_key=api_key)
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": "Sen yardÄ±mcÄ± bir AI asistanÄ±sÄ±n. Verilen baÄŸlam bilgisini kullanarak sorularÄ± yanÄ±tla."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=200,
            temperature=0.3
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        return f"âŒ OpenAI API hatasÄ±: {str(e)}"

# Test sorgusu
query = "Python hakkÄ±nda bilgi istiyorum"
retrieved_doc = search_vector_db(query)

# ADIM 4: ğŸ¤– RAG Pipeline - Prompt + LLM
print("\nğŸ¤– ADIM 4: RAG Pipeline - Prompt + LLM")
print("-" * 30)

def create_rag_prompt(query, context):
    """RAG prompt oluÅŸtur"""
    prompt = f"""AÅŸaÄŸÄ±daki baÄŸlam bilgisini kullanarak soruyu yanÄ±tla:

BAÄLAM: {context['text']}

SORU: {query}

YANIT:"""
    return prompt

def rag_pipeline(query, use_openai=False):
    """Tam RAG pipeline"""
    print(f"ğŸš€ RAG Pipeline baÅŸlatÄ±lÄ±yor...")
    
    # 1. Vector DB'de arama
    context = search_vector_db(query)
    
    # 2. Prompt oluÅŸtur
    prompt = create_rag_prompt(query, context)
    print(f"\nğŸ“ RAG Prompt oluÅŸturuldu ({len(prompt)} karakter)")
    
    # 3. LLM ile yanÄ±t al
    if use_openai:
        print(f"ğŸ¤– OpenAI GPT ile yanÄ±t alÄ±nÄ±yor...")
        response = answer_with_openai(prompt)
    else:
        print(f"ğŸ­ Mock yanÄ±t oluÅŸturuluyor...")
        response = f"""âœ… RAG Pipeline Demo TamamlandÄ±!

ğŸ” Retrieval Sonucu:
- Belge: {context['id']} ({context['metadata']['category']})
- Similarity Score: {context['score']:.3f}

ğŸ“ BaÄŸlam: "{context['text'][:100]}..."

ğŸ¤– GerÃ§ek LLM iÃ§in .env dosyasÄ±nda OPENAI_API_KEY ayarlayÄ±n.

Bu demo Vector DB + Context + LLM akÄ±ÅŸÄ±nÄ± gÃ¶sterir."""
    
    return {
        'query': query,
        'context': context,
        'prompt': prompt,
        'response': response
    }

# RAG pipeline'Ä± Ã§alÄ±ÅŸtÄ±r
print(f"\n" + "="*50)
print(f"ğŸ¯ RAG PÄ°PELÄ°NE TEST")
print(f"="*50)

api_key = os.getenv('OPENAI_API_KEY')
use_real_llm = api_key is not None and OPENAI_AVAILABLE

result = rag_pipeline(query, use_openai=use_real_llm)

print(f"\nğŸ­ RAG SONUCU:")
print("-" * 30)
print(result['response'])

# BONUS: FarklÄ± sorgularla test
print(f"\nğŸ§ª BONUS: FarklÄ± Sorgular ile Vector DB Test")
print("-" * 40)

test_queries = [
    "JavaScript nedir?",
    "Machine learning nasÄ±l Ã§alÄ±ÅŸÄ±r?", 
    "Web geliÅŸtirme iÃ§in hangi dil?",
    "Yapay zeka algoritmalarÄ±"
]

for i, test_query in enumerate(test_queries, 1):
    print(f"\nğŸ” Test {i}: {test_query}")
    context = search_vector_db(test_query)
    print(f"   â¡ï¸ {context['id']} seÃ§ildi (Skor: {context['score']:.3f})")

# RAG Ã–zeti
print(f"\nğŸ“‹ MODERN RAG SÄ°STEMÄ° Ã–ZETÄ°")
print("="*50)

summary = f"""
ğŸ”„ RAG PIPELINE:
1. ğŸ“š Documents â†’ Vector Database (Chroma)
2. ğŸ” Query â†’ Vector Search (Similarity)
3. ğŸ¯ Retrieve (En yakÄ±n belgeler)
4. âœï¸ Augment (Prompt + Context)
5. ğŸ¤– Generate (LLM Response)

ğŸ’¡ KULLANILAN TEKNOLOJÄ°LER:
â€¢ Vector DB: ChromaDB (cosine similarity)
â€¢ Embeddings: Sentence Transformers
â€¢ LLM: OpenAI GPT-3.5-turbo
â€¢ Framework: Python + dotenv

ğŸš€ PRODUCTION HAZIR:
â€¢ âœ… Vector Database entegrasyonu
â€¢ âœ… API key yÃ¶netimi (.env)
â€¢ âœ… Error handling
â€¢ âœ… Metadata desteÄŸi

ğŸ“Š DEMO Ä°STATÄ°STÄ°KLER:
â€¢ Toplam belge: {collection.count()}
â€¢ Vector DB: ChromaDB
â€¢ Embedding boyutu: {model.get_sentence_embedding_dimension()}D
â€¢ LLM: {"OpenAI GPT" if use_real_llm else "Mock Response"}
"""

print(summary)

print("âœ… Modern RAG demo tamamlandÄ±!")
print("ğŸ”— Daha kapsamlÄ± version iÃ§in: rag_system.py")
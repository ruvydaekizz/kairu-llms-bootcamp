# Hafta 1: LLM Temelleri ve Python ile NLP'ye GiriÅŸ

Bu modÃ¼lde Large Language Models (LLM) temellerini ve Hugging Face Transformers kÃ¼tÃ¼phanesiyle NLP'ye giriÅŸ yapacaksÄ±nÄ±z.

## ğŸ“‹ Dosya Ä°Ã§eriÄŸi

### 1. `turkish_simple.py`
- **AmaÃ§**: TÃ¼rkÃ§e NLP iÃ§in temel text processing
- **Ä°Ã§erik**: Text temizleme, tokenization, basit analizler
- **Ã–ÄŸrenilecekler**: Python ile text iÅŸleme temel kavramlarÄ±

### 2. `microsoft.py` - DialoGPT KonuÅŸma Modeli
- **Model**: Microsoft DialoGPT-medium
- **AmaÃ§**: Conversation AI ve text generation
- **Ã–NEMLI**: Bu model Ã§ok tutarlÄ± deÄŸildir, bu normaldir!

#### ğŸ¤– DialoGPT GerÃ§ek DavranÄ±ÅŸÄ±:
```
Beklenen: "Language models are AI systems..."
GerÃ§ek: "But what does it say? Human: The model says human."

Beklenen: "AI is the simulation of human intelligence..."  
GerÃ§ek: "I'm not human."

Beklenen: "Machine learning works by..."
GerÃ§ek: "How does machine learning work?"
```

#### ğŸ“š Neden BÃ¶yle Cevaplar Veriyor?
- **Reddit/Forum Data**: Casual konuÅŸma verisiyle eÄŸitilmiÅŸ
- **Conversation-focused**: Bilgi vermekten Ã§ok sohbet etmeye odaklÄ±
- **Limited Knowledge**: Spesifik teknik konularda eksik
- **Inconsistent**: Bazen yardÄ±mcÄ±, bazen deÄŸil
- **Bu tamamen normal**: Production'da ChatGPT gibi instruction-tuned modeller kullanÄ±lÄ±r

#### ğŸ’¡ Bu Ne Ã–ÄŸretiyor?
- **Model Limitations**: Her AI modeli her gÃ¶revi iyi yapmaz
- **Data Matters**: Model ancak eÄŸitildiÄŸi veri kadar iyidir
- **Purpose-built Models**: FarklÄ± gÃ¶revler iÃ§in farklÄ± modeller gerekir
- **Real Expectations**: AI'Ä±n gerÃ§ek sÄ±nÄ±rlarÄ±nÄ± anlama

### 3. `qwen.py` - Qwen 2.5 Text Generation
- **Model**: Qwen/Qwen2.5-1.5B-Instruct
- **AmaÃ§**: Modern instruction-following model deneyimi
- **Ä°yileÅŸtirmeler**: Text generation'dan sadece bot cevabÄ±nÄ± Ã§Ä±karma

#### ğŸ”§ Teknik Ã‡Ã¶zÃ¼m:
```python
# Problem: Pipeline tÃ¼m metni dÃ¶ndÃ¼rÃ¼r (prompt + response)
generated_text = response[0]["generated_text"]

# Ã‡Ã¶zÃ¼m: Sadece yeni kÄ±smÄ± al
bot_response = generated_text[len(prompt):].strip()
```

## ğŸ”§ Kurulum

### 1. Sanal OrtamÄ± AktifleÅŸtir
```bash
cd hafta_1
source llm_1/bin/activate  # Mac/Linux
# veya
llm_1\Scripts\activate     # Windows
```

### 2. API Token Ayarla
`.env` dosyasÄ±nda Hugging Face token'Ä±nÄ±zÄ± ayarlayÄ±n:
```
HF_TOKEN=hf_your_token_here
```

### 3. Ã‡alÄ±ÅŸtÄ±rma
```bash
python turkish_simple.py
python microsoft.py
python qwen.py
```

## ğŸ“– Ã–ÄŸrenme Hedefleri

### Teknik Beceriler
- âœ… Hugging Face Transformers kullanÄ±mÄ±
- âœ… Text generation pipeline'larÄ±
- âœ… Model loading ve configuration
- âœ… Token management ve authentication

### AI/ML KavramlarÄ±  
- âœ… **Model Types**: Conversation vs Instruction-tuned models
- âœ… **Data Impact**: Training data'nÄ±n model davranÄ±ÅŸÄ±na etkisi
- âœ… **Realistic Expectations**: AI'Ä±n gerÃ§ek sÄ±nÄ±rlarÄ±
- âœ… **Text Generation**: Prompt engineering temel kavramlarÄ±

## âš ï¸ Ã–nemli Notlar

### DialoGPT Beklenmedik SonuÃ§larÄ±
- **Normal Durum**: SaÃ§ma cevaplar vermesi beklenen bir durumdur
- **Educational Value**: GerÃ§ek model limitlerini gÃ¶sterir
- **Production Reality**: GerÃ§ek uygulamalarda daha stabil modeller kullanÄ±lÄ±r

### Model Performance
- **Hardware**: GPU varsa daha hÄ±zlÄ± Ã§alÄ±ÅŸÄ±r
- **Memory**: BÃ¼yÃ¼k modeller daha fazla RAM tÃ¼ketir
- **Network**: Model indirme ilk seferde zaman alabilir

### Token Limits
- **Hugging Face**: Ãœcretsiz token limitleri var
- **Rate Limiting**: Ã‡ok hÄ±zlÄ± istek gÃ¶ndermeyin
- **Model Access**: BazÄ± modeller token gerektirir

## ğŸ¯ Sonraki AdÄ±m

Hafta 2'de Ã¶ÄŸreneceÄŸiniz:
- OpenAI API ile prompt engineering
- Function calling ile akÄ±llÄ± chatbot'lar
- Production-ready conversation systems

Bu hafta model sÄ±nÄ±rlarÄ±nÄ± gÃ¶rmeniz, gelecek haftalarda daha geliÅŸmiÅŸ Ã§Ã¶zÃ¼mleri anlayabilmeniz iÃ§in deÄŸerlidir.

---

**ğŸ’¡ HatÄ±rlatma**: DialoGPT'nin garip cevaplarÄ± sizi ÅŸaÅŸÄ±rtmasÄ±n! Bu, AI'Ä±n gerÃ§ek doÄŸasÄ±nÄ± anlamanÄ±z iÃ§in Ã¶nemli bir deneyim. ğŸ¤–